################################################################################
# PATCH #2: FIX RACE CONDITION IN JOB START
# Priority: CRITICAL
# Time to apply: 10 minutes
################################################################################

DESCRIPTION:
Prevents duplicate active jobs when multiple users try to start jobs simultaneously.
Uses database-level locking to ensure atomicity.

FILES TO MODIFY:
1. main.py

DEPENDENCIES:
- Patch #1 (Logging) should be applied first for proper error tracking

################################################################################
# CHANGES TO main.py
################################################################################

# ADD IMPORT AT TOP
# ===========================================================================
# ADD after other sqlmodel imports:

from sqlalchemy.exc import OperationalError
import time


# COMPLETELY REPLACE start_job ENDPOINT
# ===========================================================================
# FIND:
@app.post("/api/job/start")
async def start_job(request: JobStartRequest, session: Session = Depends(get_session)):
    # Check active
    active = session.exec(select(Job).where(Job.is_active == True)).first()
    if active:
        return JSONResponse(status_code=400, content={'error': 'A job is already active. End it first.'})
    
    if not request.expected_barcode.strip():
        return JSONResponse(status_code=400, content={'error': 'Expected barcode is required'})
    
    job_id = request.job_id
    if not job_id or not job_id.strip():
        job_id = datetime.now().strftime('JOB_%Y%m%d_%H%M%S')
        
    job = Job(
        job_id=job_id,
        expected_barcode=request.expected_barcode,
        pieces_per_shipper=max(1, request.pieces_per_shipper),
        target_quantity=max(0, request.target_quantity),
        start_time=datetime.now(),
        is_active=True
    )
    session.add(job)
    session.commit()
    session.refresh(job)
    
    job_read = JobRead.from_job(job)
    await notify_clients('job_started', job_read.model_dump())
    
    return {'success': True, 'summary': job_read}

# REPLACE WITH:
@app.post("/api/job/start")
async def start_job(request: JobStartRequest, session: Session = Depends(get_session)):
    logger.info(f"Job start request: job_id={request.job_id}, barcode={request.expected_barcode[:20]}...")
    
    # Validate input
    if not request.expected_barcode.strip():
        logger.warning("Job start failed: empty barcode")
        return JSONResponse(status_code=400, content={'error': 'Expected barcode is required'})
    
    # Generate job_id if not provided
    job_id = request.job_id
    if not job_id or not job_id.strip():
        job_id = datetime.now().strftime('JOB_%Y%m%d_%H%M%S')
    
    max_retries = 3
    for attempt in range(max_retries):
        try:
            # Use nested transaction for atomic check-and-create
            session.begin_nested()
            
            # Check for active job with SELECT FOR UPDATE
            # This locks the row(s) to prevent race conditions
            active = session.exec(
                select(Job)
                .where(Job.is_active == True)
                .with_for_update()
            ).first()
            
            if active:
                session.rollback()
                logger.warning(f"Cannot start job - active job exists: {active.job_id}")
                return JSONResponse(
                    status_code=400, 
                    content={'error': f'A job is already active: {active.job_id}. End it first.'}
                )
            
            # No active job - create new one
            job = Job(
                job_id=job_id,
                expected_barcode=request.expected_barcode,
                pieces_per_shipper=max(1, request.pieces_per_shipper),
                target_quantity=max(0, request.target_quantity),
                start_time=datetime.now(),
                is_active=True
            )
            session.add(job)
            session.commit()
            session.refresh(job)
            
            logger.info(f"Job started successfully: job_id={job.job_id}, id={job.id}")
            
            # Notify connected clients
            job_read = JobRead.from_job(job)
            await notify_clients('job_started', job_read.model_dump())
            
            return {'success': True, 'job': job_read}
            
        except OperationalError as e:
            # Handle database locked errors (SQLite specific)
            if "database is locked" in str(e) and attempt < max_retries - 1:
                logger.warning(f"Database locked, retry {attempt + 1}/{max_retries}")
                session.rollback()
                time.sleep(0.1 * (attempt + 1))  # Exponential backoff
                continue
            else:
                logger.error(f"Failed to start job after {attempt + 1} attempts: {e}", exc_info=True)
                session.rollback()
                return JSONResponse(
                    status_code=500, 
                    content={'error': 'Database error. Please try again.'}
                )
        except Exception as e:
            logger.error(f"Unexpected error starting job: {e}", exc_info=True)
            session.rollback()
            return JSONResponse(
                status_code=500, 
                content={'error': 'Failed to start job. Please try again.'}
            )
    
    # Should never reach here, but just in case
    logger.error("Failed to start job after all retries")
    return JSONResponse(
        status_code=500, 
        content={'error': 'Failed to start job after multiple attempts'}
    )


################################################################################
# OPTIONAL: ADD UNIQUE CONSTRAINT (RECOMMENDED)
################################################################################

# This provides database-level enforcement of the "one active job" rule.
# However, it requires a database migration.

# ADD TO models.py:
# ===========================================================================
# FIND in Job class:
class Job(SQLModel, table=True):
    __tablename__ = "jobs"
    
    id: Optional[int] = Field(default=None, primary_key=True)
    # ... other fields ...

# ADD after the field definitions, before the methods:
    
    __table_args__ = (
        # Ensure only one active job at a time
        # Note: This creates a partial index that only includes active jobs
        # SQLite 3.8.0+ required
        Index(
            'idx_single_active_job',
            'is_active',
            unique=True,
            sqlite_where=text('is_active = 1')
        ),
    )

# ADD this import at top of models.py:
from sqlalchemy import Index, text


################################################################################
# MIGRATION SCRIPT (if adding unique constraint)
################################################################################

# Create file: migrate_add_active_job_constraint.py
# ===========================================================================

"""
Run this once to add the unique constraint to existing database.
WARNING: This will fail if you have multiple active jobs!
"""

from sqlmodel import Session, select
from database import engine
from models import Job

def migrate():
    with Session(engine) as session:
        # First, ensure only one active job
        active_jobs = session.exec(select(Job).where(Job.is_active == True)).all()
        
        if len(active_jobs) > 1:
            print(f"ERROR: Found {len(active_jobs)} active jobs!")
            print("Please manually end all but one job before running migration.")
            print("Active jobs:")
            for job in active_jobs:
                print(f"  - {job.job_id} (ID: {job.id})")
            return False
        
        # If SQLite version supports partial indexes, the constraint
        # will be created automatically when the model is loaded
        print("Migration complete!")
        print(f"Active jobs: {len(active_jobs)}")
        return True

if __name__ == "__main__":
    migrate()


################################################################################
# TESTING
################################################################################

Test the race condition fix:

1. Open two browser windows to http://localhost:8000
2. In both windows simultaneously:
   - Enter different job IDs
   - Enter same barcode
   - Click "START JOB" at the exact same time
3. Expected result: Only ONE job starts, the other gets error message

To test programmatically:

```python
import requests
import threading

def start_job():
    response = requests.post('http://localhost:8000/api/job/start', json={
        'expected_barcode': 'TEST123',
        'pieces_per_shipper': 1
    })
    print(f"Status: {response.status_code}")

# Start 10 jobs simultaneously
threads = [threading.Thread(target=start_job) for _ in range(10)]
for t in threads:
    t.start()
for t in threads:
    t.join()

# Should see: 1 success (200), 9 failures (400)
```

################################################################################
# ROLLBACK
################################################################################

To rollback:
1. Revert main.py changes (restore original start_job endpoint)
2. If you added the unique constraint, remove it from models.py
3. Restart the application

Note: The unique constraint in the database will persist unless you:
- Delete the database file, OR
- Manually drop the index: DROP INDEX IF EXISTS idx_single_active_job;
