################################################################################
# PATCH #4: FIX PERFORMANCE - CACHED COUNTS
# Priority: HIGH
# Time to apply: 20 minutes
################################################################################

DESCRIPTION:
Replaces slow in-memory count calculations with database-cached values.
Current code scans all scans in Python on every access - extremely slow with
many scans. This patch adds cached columns updated on each scan.

FILES TO MODIFY:
1. models.py
2. main.py
3. Create migration script

PERFORMANCE IMPACT:
- Before: O(n) for every property access where n = number of scans
- After: O(1) - simple column lookup

################################################################################
# STEP 1: MODIFY models.py
################################################################################

# FIND the Job class:
class Job(SQLModel, table=True):
    __tablename__ = "jobs"
    
    id: Optional[int] = Field(default=None, primary_key=True)
    job_id: str = Field(max_length=100)
    expected_barcode: str = Field(max_length=200)
    pieces_per_shipper: int = Field(default=1)
    target_quantity: int = Field(default=0)
    start_time: datetime = Field(default_factory=datetime.now)
    end_time: Optional[datetime] = Field(default=None)
    is_active: bool = Field(default=True)
    
    scans: List["Scan"] = Relationship(back_populates="job")

# REPLACE WITH (add cached fields):
class Job(SQLModel, table=True):
    __tablename__ = "jobs"
    
    id: Optional[int] = Field(default=None, primary_key=True)
    job_id: str = Field(max_length=100)
    expected_barcode: str = Field(max_length=200)
    pieces_per_shipper: int = Field(default=1)
    target_quantity: int = Field(default=0)
    start_time: datetime = Field(default_factory=datetime.now)
    end_time: Optional[datetime] = Field(default=None)
    is_active: bool = Field(default=True)
    
    # NEW: Cached counts for performance
    cached_pass_count: int = Field(default=0)
    cached_fail_count: int = Field(default=0)
    cached_total_scans: int = Field(default=0)
    
    scans: List["Scan"] = Relationship(back_populates="job")


# UPDATE the properties to use cached values:
# FIND:
    @property
    def pass_count(self) -> int:
        return len([s for s in self.scans if s.status == 'PASS'])
    
    @property
    def fail_count(self) -> int:
        return len([s for s in self.scans if s.status == 'FAIL'])
    
    @property
    def total_scans(self) -> int:
        return len(self.scans)
    
    @property
    def total_pieces(self) -> int:
        return self.pass_count * self.pieces_per_shipper

# REPLACE WITH:
    @property
    def pass_count(self) -> int:
        """Use cached count for performance"""
        return self.cached_pass_count
    
    @property
    def fail_count(self) -> int:
        """Use cached count for performance"""
        return self.cached_fail_count
    
    @property
    def total_scans(self) -> int:
        """Use cached count for performance"""
        return self.cached_total_scans
    
    @property
    def total_pieces(self) -> int:
        """Calculate from cached pass count"""
        return self.cached_pass_count * self.pieces_per_shipper


# KEEP the hourly scan methods but update them to be more efficient:
# FIND:
    def scans_in_hour(self, target_hour: int) -> int:
        # This is a bit inefficient in Python memory but simplest for migration
        today = datetime.now().date()
        start_dt = datetime.combine(today, datetime.min.time().replace(hour=target_hour))
        end_dt = start_dt + timedelta(hours=1)
        
        count = 0
        for s in self.scans:
            if s.status == 'PASS' and start_dt <= s.timestamp < end_dt:
                count += 1
        return count

# REPLACE WITH (use database query instead):
    def scans_in_hour(self, target_hour: int, session=None) -> int:
        """Count scans in a specific hour using database query for performance"""
        if not session:
            # Fallback to memory scan if no session provided
            today = datetime.now().date()
            start_dt = datetime.combine(today, datetime.min.time().replace(hour=target_hour))
            end_dt = start_dt + timedelta(hours=1)
            return len([s for s in self.scans if s.status == 'PASS' and start_dt <= s.timestamp < end_dt])
        
        # Use database query (much faster)
        from sqlmodel import select, func
        today = datetime.now().date()
        start_dt = datetime.combine(today, datetime.min.time().replace(hour=target_hour))
        end_dt = start_dt + timedelta(hours=1)
        
        count = session.exec(
            select(func.count(Scan.id))
            .where(Scan.job_id == self.id)
            .where(Scan.status == 'PASS')
            .where(Scan.timestamp >= start_dt)
            .where(Scan.timestamp < end_dt)
        ).one()
        
        return count or 0


################################################################################
# STEP 2: UPDATE main.py - process_scan endpoint
################################################################################

# FIND:
@app.post("/api/scan", response_model=ScanResultResponse)
async def process_scan(request: ScanRequest, session: Session = Depends(get_session)):
    # ... existing code that creates scan ...
    
    scan = Scan(
        job_id=job.id,
        barcode=barcode,
        expected=job.expected_barcode,
        status=status,
        timestamp=datetime.now()
    )
    session.add(scan)
    session.commit()
    session.refresh(scan)
    session.refresh(job)  # Refresh job to update relationship/counts

# REPLACE WITH (update cached counts):
@app.post("/api/scan", response_model=ScanResultResponse)
async def process_scan(request: ScanRequest, session: Session = Depends(get_session)):
    # ... existing validation code ...
    
    barcode = request.barcode.strip()
    logger.debug(f"Scan request: barcode={barcode[:20]}...")
    
    try:
        if not barcode:
            logger.warning("Empty barcode received")
            return JSONResponse(status_code=400, content={'error': 'No barcode provided'})
        
        job = session.exec(select(Job).where(Job.is_active == True)).first()
        if not job:
            logger.error("Scan attempted with no active job")
            return JSONResponse(status_code=400, content={'error': 'No active job'})
        
        status = 'PASS' if barcode == job.expected_barcode else 'FAIL'
        logger.info(f"Scan processed: job={job.job_id}, status={status}, barcode={barcode[:20]}...")
        
        # Create scan record
        scan = Scan(
            job_id=job.id,
            barcode=barcode,
            expected=job.expected_barcode,
            status=status,
            timestamp=datetime.now()
        )
        session.add(scan)
        
        # UPDATE CACHED COUNTS - THIS IS THE KEY CHANGE
        job.cached_total_scans += 1
        if status == 'PASS':
            job.cached_pass_count += 1
        else:
            job.cached_fail_count += 1
        
        session.add(job)
        session.commit()
        session.refresh(scan)
        session.refresh(job)
        
        # Trigger GPIO
        if status == 'PASS':
            gpio_controller.trigger_pass()
        else:
            gpio_controller.trigger_fail()
        
        # Prepare response
        response_data = ScanResultResponse(
            scan=ScanRead.from_scan(scan),
            job=JobRead.from_job(job),
            recent_scans=[ScanRead.from_scan(s) for s in job.recent_scans(8)]
        )
        
        await notify_clients('scan', json.loads(response_data.model_dump_json()))
        
        return response_data
        
    except Exception as e:
        logger.error(f"Scan processing failed: {e}", exc_info=True)
        session.rollback()
        raise


################################################################################
# STEP 3: CREATE MIGRATION SCRIPT
################################################################################

# Create new file: migrate_add_cached_counts.py

from sqlmodel import Session, select, SQLModel
from database import engine
from models import Job, Scan
import sys

def migrate_add_cached_counts():
    """
    Migration script to add cached count columns and populate them.
    
    This script:
    1. Creates the new columns (if not already present)
    2. Calculates correct counts for all existing jobs
    3. Updates all job records
    """
    
    print("Starting migration: Add cached counts to Job table")
    print("=" * 60)
    
    try:
        # Create all tables (will add new columns)
        SQLModel.metadata.create_all(engine)
        print("✓ Database schema updated")
        
        with Session(engine) as session:
            # Get all jobs
            jobs = session.exec(select(Job)).all()
            print(f"Found {len(jobs)} jobs to migrate")
            
            if not jobs:
                print("No jobs found - migration complete")
                return True
            
            # Update each job
            updated_count = 0
            for i, job in enumerate(jobs, 1):
                # Calculate actual counts from scans
                pass_count = len([s for s in job.scans if s.status == 'PASS'])
                fail_count = len([s for s in job.scans if s.status == 'FAIL'])
                total_count = len(job.scans)
                
                # Update cached values
                job.cached_pass_count = pass_count
                job.cached_fail_count = fail_count
                job.cached_total_scans = total_count
                
                session.add(job)
                updated_count += 1
                
                if i % 10 == 0:
                    print(f"  Progress: {i}/{len(jobs)} jobs processed")
            
            # Commit all changes
            session.commit()
            print(f"✓ Updated {updated_count} jobs")
            
            # Verify
            print("\nVerification:")
            sample_jobs = session.exec(select(Job).limit(5)).all()
            for job in sample_jobs:
                print(f"  Job {job.job_id}: "
                      f"pass={job.cached_pass_count}, "
                      f"fail={job.cached_fail_count}, "
                      f"total={job.cached_total_scans}")
            
            print("\n" + "=" * 60)
            print("Migration completed successfully!")
            print("=" * 60)
            return True
            
    except Exception as e:
        print(f"\n❌ Migration failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = migrate_add_cached_counts()
    sys.exit(0 if success else 1)


################################################################################
# STEP 4: RUN MIGRATION
################################################################################

# After applying code changes:

1. STOP the application
2. Run migration:
   
   uv run python migrate_add_cached_counts.py

3. Expected output:
   Starting migration: Add cached counts to Job table
   ============================================================
   ✓ Database schema updated
   Found X jobs to migrate
   ✓ Updated X jobs
   
   Verification:
     Job JOB_20241203_100000: pass=150, fail=2, total=152
     ...
   ============================================================
   Migration completed successfully!
   ============================================================

4. START the application
5. Verify everything works


################################################################################
# TESTING
################################################################################

Test performance improvement:

# Before patch (slow):
# Each job.pass_count access scans ALL scans in memory

# After patch (fast):
# Each job.pass_count is a simple database column read

# To measure:
import time
from models import Job
from database import engine
from sqlmodel import Session, select

with Session(engine) as session:
    job = session.exec(select(Job).where(Job.is_active == True)).first()
    
    # Test performance
    start = time.time()
    for i in range(1000):
        _ = job.pass_count
        _ = job.fail_count
        _ = job.total_scans
    elapsed = time.time() - start
    
    print(f"1000 property accesses: {elapsed:.4f} seconds")
    # Should be near instant (<0.01s) with cached counts
    # Would be much slower with old implementation if job has many scans


################################################################################
# ROLLBACK
################################################################################

To rollback (if issues occur):

1. STOP the application

2. Revert models.py:
   - Remove cached_* fields
   - Restore original @property implementations

3. Revert main.py:
   - Remove the cached count updates in process_scan

4. The cached columns will remain in database but unused (harmless)
   
5. To fully remove columns (optional):
   sqlite3 barcode_verification.db
   ALTER TABLE jobs DROP COLUMN cached_pass_count;
   ALTER TABLE jobs DROP COLUMN cached_fail_count;
   ALTER TABLE jobs DROP COLUMN cached_total_scans;
   
   Note: SQLite may not support DROP COLUMN in older versions

6. START the application


################################################################################
# NOTES
################################################################################

- This patch is backward compatible - old data will work after migration
- Migration is idempotent - safe to run multiple times
- Cached counts are always correct as long as scans go through the API
- If scans are added directly to database, counts will be wrong
- Consider adding a "recalculate counts" admin endpoint for safety
